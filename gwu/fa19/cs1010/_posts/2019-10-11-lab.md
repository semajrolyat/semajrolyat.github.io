---
layout: post
title:  "Machine Learning"
date:   2019-10-11 00:00:00 -0400
schedule:   2019-10-11 00:00:00 -0400
categories: [GWU]
docclass: "lab"
gwclass: "cs1010"
term: "fa19"
---
<head>
  <link href="/css/syntax.css" rel="stylesheet">
</head>

# Lab 4 - Machine Learning

## Introduction

Machine learning is defined as teaching machines to imitate and adapt human like behavior. The objective here is for the machine to think, talk, and reason like humans.

As a human, if you see the following quiz:
1. 3-9
2. 4-16
3. 8-64
4. 9-?

What would you put to replace the question mark?

The answer is 81, because humans are able to recognize patterns and are informed by abstract ideas developed through learning.


## What can an intelligent machine do for humans?

### Voice Assistance

Perhaps you have used Siri (or Google assistant) in your smart phones to ask your questions or to do certain functionality such as sending text messages, or setup an event in your calendar, etc. Have you considered how Siri does what it does?

Let's take an example:
- Hi Siri
- Hi there, John
- What is the weather today?
- The weather is looking good today, up to 68 degree

Can you ask this question differently and get the same response?

Of course! Siri is an intelligent system that can recognize and understand human natural language in form of speech and provide assistant base on its understanding.  

However, Siri does have significant problems with interpreting natural language.  For example, consider the following video:

<a href="https://www.youtube.com/watch?v=P8zG_gGmfmo" target="_blank"><img src="https://i.ytimg.com/vi/P8zG_gGmfmo/maxresdefault.jpg"
alt="5 year old tries to have a conversation with Siri" width="640" height="360" border="10" /></a>

### Amazon Recommendations

Another example is Amazon search service. Perhaps you have noted the recommended items when you search for an item in Amazon website.

For instance, when I searched for "Human + Machine: Reimagining Work in the Age of AI", my Amazon page recommended the following books: "Pivot to the Future", "Prediction Machines", "AI Super Power".

This sounds a lot like a trusted adviser, maybe a close friend who knows what could be interesting for you or a knowledgeable person who knows what people mostly are buying together.  This adviser in Amazon is an intelligent system which is capable of acting like your best friend or a knowledgeable person.  

## A Scenario - Should I Go See That Movie?

Suppose this Saturday you have some free time and you want to go to movies. There is a new movie just came out and you want to watch it, however, you do not know if it is worth your time and money.  

To inform your decision, you might talk to friends or family and get their opinions.  However, you find that your tastes in movies do not always align well with their tastes in movies.

You might also look at online resources, like IMDB or Rotten Tomatoes, to help make your decision, but you don't always agree with these resources for movies that are not outright blockbusters.  In fact, some of you favorite movies have somewhat ambiguous ratings on these sites.

Another way to help determine whether you might enjoy this movie is to analyze sentiment from a large number of people who have seen the movie.  A small number of reviewers might be biased, but surely there is a way to sample the overall sentiment towards this movie from a large population.  The trailer looks good enough and you have enjoyed a number of movies from this director that you decide if the overall sentiment toward the movie is positive, then its probably worth your time.

To solve this problem you scrape available reviews from Rotten Tomatoes over a broad number of movies and look for a way to the classify sentiment expressed in the reviews.

## Sentiment Analysis

Evaluating the overall sentiment from a large set of natural language is called sentiment analysis.  To perform this analysis, researches build an intelligent system which is capable to analyze people sentiments.

In the following step by step instruction, we will use a python library to build a very simple system that is able to classify sentiment of strangers who wrote a reviews about different movies on the Rotten Tomatoes website.

## Fundamental Tasks

In order to build any machine learning system, we need to perform a number of tasks.

* First, we need data.  Machine learning systems are trained on a large data set.  We need a sufficient number of representative samples in order to train a system.  The question is what is a sufficient amount of data.  Due to the complexity of natural language, a good sentiment analysis machine learning model requires massive amounts of data.

* Second, we need to process the data in such a way that it is useful for a machine learning system to consume.  For natural language, we may need a human in the loop to clean up the text so that it does not contain characters that might cause problems when fed into an automated system.  We might also need to interpret and quantify the data a bit so that the machine learning system has an objective measure by which it may classify the data.

* Third, we need to select an appropriate machine learning algorithm that can operate on the data and then develop a program that generates a machine learning model.

* Fourth, we need to train the model.

* Fifth, we need to test the model to gauge its accuracy.  If the model is too inaccurate, than any results it produces are ambiguous an nonsensical.  At the minimum, we would like for our model to produce results that are better than a random guess.  If an intelligent system produces an accuracy no better than 50%, than that system is no better than a random guess (a coin flip).  If the accuracy is poor, then we may need to reevaluate our data, methods, parameters, and whether or not this is even a valid application of machine learning.

* Sixth, if we make it to this step, we might be able to use the intelligent system to answer our questions.  However, we need to be very careful to place too much trust in the results.  There are a number of significant issues that we may have ignored or overlooked when developing our approach and these factors can heavily affect the validity of results produced by the system.  In fact, we may not even be able to determine if a seemingly high accuracy result is a viable and meaningful answer due to the complexity of these systems.

## Building a System to Analyze Sentiment in Text

To analyze sentiment in text we need to be able to categorize the text into the sentiment categories we have. Currently, the two broad sentiment categories are "positive" sentiment and "negative" sentiment, we have more fine-grained categories such as: very negative, negative, neutral, positive, very positive.

Defining the right set of categories is another challenge in this problem which is outside of the scope of this tutorial.  For the sake of this exercise, let's assume that a little birdy has classified the sentiment expressing in movie reviews for us.  

Sentiment analysis is a form of text classification, in which, we train a model to separate these sentiments from each other.

After reading this toturial you will know:

* How to develop a model for text classification
* How to train a classifier model
* How to use the trained model in future to analyze any text

Let's get started.

### Obtaining Training and Test Data

To train any text classifier you need to have labeled data, which in our case we should have movie reviews that are labeled with: very negative, negative, neutral, positive, very positive. We are lucky that the little birdy has done that for us and we have labeled movie reviews.

Create a folder for this lab on your computer and download the following data files into that folder:
* train: [```movie_reviews_train.txt```]({{ "/gwu/fa19/cs1010/assets/lab4/movie_reviews_train.txt" | absolute_url }}),
* test: [```movie_reviews_test.txt```]({{ "/gwu/fa19/cs1010/assets/lab4/movie_reviews_test.txt" | absolute_url }})

There are a total of 8544 movie reviews are in the training dataset, _i.e._ ```movie_reviews_train.txt```, and 1101 movie reviews in testing dataset.  The columns in these files are tab separated:
* columns 1 is the movie review
* columns 2 is the label of the review in numeric form:
  * -2: very negative, -1: negative, 0: neutral, 1: positive, 2: very positive

We need to read those text files into our program and save reviews into one array and save the sentiment labels into another array. The following code does that for us:

```python
print("Reading the tab delimited files ...")

dataset_x = []
dataset_y = []
trainfile = './movie_reviews_train.txt'
testfile = './movie_reviews_test.txt'

train = open(trainfile, encoding="utf8")
test = open(testfile, encoding="utf8")

// to keep the count of training set
index = 0
print("Get the lines from the tab delimited file & split the line to its parts, tweet text is the 2nd part")
for line in train:
    parts = line.split('\t')
    dataset_x.append(parts[0].replace('\n', ''))
    dataset_y.append(parts[1].replace('\n', ''))
    index += 1

for line in test:
    parts = line.split('\t')
    dataset_x.append(parts[0].replace('\n', ''))
    dataset_y.append(parts[1].replace('\n', ''))
```

### Vectorizing Text

Machines can only recognize and understand numeric data as the representation of text.  Hence, we need to represent each text in the form of a numeric to train a text classifier.  For that we use words content in the text.  We can take one word or combination of two, three, four, .... words to represent a text.

For example: I enjoy coding with Python.

One word: I, enjoy, coding, with, python, .
Two words: I enjoy, enjoy coding, coding with, with python, python .
Three words: I enjoy coding, enjoy coding with, coding with python, with python .

Each of the above combination can be used to represent the sentence and is called an n-gram, where n is the number of combinations. A 1-gram called uni-gram, a 2-gram called bi-gram, etc.  We simply classify each sentence by presence or absence of each of the n-grams over the entire training set.  If our training set has 1000 words in total, each sentence is represented by 1000 numbers which has value of 0 or 1, we call these numbers uni-gram feature representations.

Python ```sklearn``` library creates n-grams features for you. The following piece of code does the magic for you:

```python
from sklearn.feature_extraction.text import CountVectorizer
// n-grams max and min
ngram_min = 1
ngram_max = 1

print("Vectorizing Corpus ...")
vectorizer = CountVectorizer(ngram_range=(ngram_min, ngram_max), token_pattern=r'\b\w+\b', min_df=1)
print("Fit it to the data or convert each text to 0s and 1s ...")
X_fit = vectorizer.fit_transform(dataset_x)
print("Transform the 0s and 1s to an array for each text ...")
dataset_x = vectorizer.transform(dataset_x).toarray()

Once done, you need to separate training set from test set and save them in separate arrays:

// Separate train set from test, -1 is for training set offset

x_train = dataset_x[:index-1]
y_train = dataset_y[:index-1]

x_test = dataset_x[index:]
y_test = dataset_y[index:]
```

### Training a Text Classifier

A text classifier is a function which can analyze and determine the correlation among feature representations in text.  For example, if "great movie" happened many times in reviews with a very positive label, then the classifier learns that this is an indicator for very positive.  We will not teach in this tutorial the math under the hood of the text classifier, however the ```sklearn``` library has a short and clear explanation about each classifier.  

There are various number of classifiers developed in ```sklearn``` package, among those we use the fast one to demonstrate this task.  However, the classifier we demonstrate here is not the best option for sentiment analysis.  We will use "MultinomialNB" from Naive Bayes classification.  

For obtaining better results you might use one of the SVM family or you might develop your own more sophisticated model using the ```pytorch``` or ```tensorflow``` libraries.  For these approaches, you probably need to craft more features from the text.  Sentiment analysis is a challenging task and needs deep understanding of linguistic and computational linguistic which are outside of the scope of this tutorial.

The following code is training "MultinomialNB" with the training set, after training the model is able to produce sentiment labels:

```python
from sklearn.naive_bayes import MultinomialNB
print("Training models ... \n")
model = MultinomialNB().fit(x_train, y_train)
```

This could take a while, if you decide to use SVM, training can take a long time. Without computing power like a GPU, _i.e._ a Graphics Processing Unit or a modern video card, it could take weeks to train depending on the size of the data.

The following is the code for training a SVM classifier:

```python
from sklearn.svm import SVC
print("Training models ... \n")
model = SVC(gamma='auto').fit(x_train, y_train)
```
### Evaluating a Text Classifier

Once you have trained a model, you need to evaluate the performance of the model, just like a teacher that gives quizzes and exams as a way to evaluate student performance, teachers of a classifier model must do the same.  To test the model, we must design an exam for that model and evaluate the results.  

The metric for a classifier model is different from say a final/midterm/quiz evaluated with 100 points.  For us it is important to know: first, how many data points (reviews in our case) are correct per label;  second, how many labels are incorrectly classified; last, if labels are incorrectly classified, then which labels are confused with one another.  All that may sound complicated, but it can be simple math equation which gives a metric called accuracy.  Accuracy is the percentage that quantifies how accurate the model produces the labels. Again, this is another magic library in ```sklearn``` and it is calculated using this code:

```python
print('Mean Accuracy: ' + str(float("{0:.3f}".format(model.score(x_test, y_test)))))
```

The above code evaluate the accuracy of the model you just built on the "quiz" set which is the test set.  To evaluate the model, you need to have a fresh set of reviews different from what your model is trained with.  This is an important factor that you need to carefully consider for the fair evaluation of the model.  

In summary, we want to evaluate how much the model is able to generalize the task.

### Save the Model to Disk

Training a sophisticated model can be time consuming, hence, it is beneficial to save the model to disk for later use. The following method in ```sklearn``` saves the model to a file and load it for later use on test set or other unseen data.

```python
from sklearn.externals import joblib
filename = 'finalized_model.sav'
joblib.dump(model, filename)

//some time later...

//load the model from disk

loaded_model = joblib.load(filename)
result = loaded_model.score(x_test, y_test)
print('Mean Accuracy: ' + str(float("{0:.3f}".format(result))))
```

### Visualizing Results with ```matplotlib```

Visualizing a classifier results is a great way to be able to gain knowledge about the performance. We do not get very fancy here, so we use ```matplotlib``` to show the distribution of the predicted labels by the model. We can be selective and see only a few of the results.  Here we show the label predicted by the model for the first 15 data points from test set.

The following code will allow us to plot the classifier results:

```python
xt = x_test[1:15]
plt.figure()
plt.plot(clf.predict(xt), 'gd', label='MultiNB')
plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)
plt.ylabel('predicted')
plt.xlabel('training samples')
plt.legend(loc="best")
plt.title('Comparison of individual predictions with averaged')
plt.show()
```

You can download the full code from here: [ml.py]({{ "/gwu/fa19/cs1010/assets/lab4/ml.py" | absolute_url }})
